{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범용 라이브러리\n",
    "import pandas as pd     # DataFrame, Series 및 데이터 분석\n",
    "import numpy as np      # Array 및 \n",
    "from tqdm import tqdm   # 진행상황 Progress Bar를 위한 tqdm library\n",
    "import re               # Regular Expression 사용\n",
    "import pickle           # 토큰화된 단어목록의 인덱스를 저장, 불러오기 위해 사용\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 토크나이징, 인코딩 관련 라이브러리\n",
    "from eunjeon import Mecab           # Mecab 형태소 분석기의 한국어+윈도우용 버전인 은전한닢 프로젝트\n",
    "from collections import Counter     \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 그래프 관련 라이브러리\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import platform\n",
    "\n",
    "# 한글 폰트 설정\n",
    "if (platform.system() == 'Windows'):\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "\n",
    "# 음수(-)가 깨지는 현상 방지\n",
    "plt.rcParams['axes.unicode_minus'] = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국표준산업분류 딕셔너리 불러오기\n",
    "\n",
    "with open('./data/dictionary/digit_1_dict.pickle', 'rb') as handle:\n",
    "    digit_1_dict = pickle.load(handle)\n",
    "\n",
    "with open('./data/dictionary/digit_2_dict.pickle', 'rb') as handle:\n",
    "    digit_2_dict = pickle.load(handle)\n",
    "\n",
    "with open('./data/dictionary/digit_3_dict.pickle', 'rb') as handle:\n",
    "    digit_3_dict = pickle.load(handle)\n",
    "\n",
    "# 레이블 인코딩을 위한 산업분류 리스트, 데이터 프레임 만들기\n",
    "\n",
    "digit_1_list = list(digit_1_dict.keys())\n",
    "digit_1_df = pd.DataFrame([], columns=['digit_1'], index=[0])\n",
    "for i in range(0, len(digit_1_dict)):\n",
    "    digit_1_df.loc[i, 'digit_1'] = digit_1_list[i]\n",
    "\n",
    "digit_2_list = list(digit_2_dict.keys())\n",
    "digit_2_df = pd.DataFrame([], columns=['digit_2'], index=[0])\n",
    "for i in range(0, len(digit_2_dict)):\n",
    "    digit_2_df.loc[i, 'digit_2'] = digit_2_list[i]\n",
    "\n",
    "digit_3_list = list(digit_3_dict.keys())\n",
    "digit_3_df = pd.DataFrame([], columns=['digit_3'], index=[0])\n",
    "for i in range(0, len(digit_3_dict)):\n",
    "    digit_3_df.loc[i, 'digit_3'] = digit_3_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:42<00:00, 14.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# 구두점, 오타(ㅋ, ㅡ 등 한글자) 제거\n",
    "\n",
    "for col in tqdm(train[['text_obj', 'text_mthd', 'text_deal']].columns):\n",
    "    train[col] = train[col].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-z ]',' ', regex=True).str.replace('[* , .]', ' ', regex=True)\n",
    "    test[col] = test[col].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-z ]','', regex=True).str.replace('[* , .]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치를 \"\"로 대치 \n",
    "\n",
    "train.fillna(\"\",inplace=True)\n",
    "test.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        고\n",
      "1        업\n",
      "2        아\n",
      "3        휴\n",
      "4      아이구\n",
      "      ... \n",
      "661      원\n",
      "662      잘\n",
      "663     통하\n",
      "664     소리\n",
      "665      놓\n",
      "Name: stopwords, Length: 666, dtype: object 666\n"
     ]
    }
   ],
   "source": [
    "# 작성해둔 한국어 불용어 사전 불러오기\n",
    "\n",
    "stopwords = pd.read_csv(\"./data/stopwords/stopwords.csv\", encoding='CP949')\n",
    "stopwords = stopwords['stopwords']\n",
    "print(stopwords, len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터\n",
    "train['text'] = train['text_obj'] + train['text_mthd'] + train['text_deal']\n",
    "# test 데이터\n",
    "test['text']  = test['text_obj'] + test['text_mthd'] + test['text_deal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = \"[CLS] \" + train['text'] + \" [SEP]\"\n",
    "\n",
    "test['text'] = \"[CLS] \" + test['text'] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 243k/243k [00:00<00:00, 290kB/s]  \n",
      "Downloading: 100%|██████████| 125/125 [00:00<00:00, 41.6kB/s]\n",
      "Downloading: 100%|██████████| 289/289 [00:00<00:00, 57.8kB/s]\n",
      "Downloading: 100%|██████████| 425/425 [00:00<00:00, 106kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = pd.DataFrame(train['digit_1'])\n",
    "y_train_2 = pd.DataFrame(train['digit_2'])\n",
    "y_train_3 = pd.DataFrame(train['digit_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-718fbd3579a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mohe2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigit_2_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0my_train_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# ohe2 객체에 담긴 인코딩 정보가 ohe2.pickle에 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown,\n\u001b[0m\u001b[0;32m    462\u001b[0m                                         force_all_finite='allow-nan')\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, handle_unknown, force_all_finite)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             diff, valid_mask = _check_unknown(Xi, self.categories_[i],\n\u001b[0m\u001b[0;32m    130\u001b[0m                                               return_mask=True)\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\u001b[0m in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# check for nans in the known_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknown_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[0mdiff_is_nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdiff_is_nan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# 대분류 원-핫 인코딩\n",
    "\n",
    "ohe1 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe1.fit(digit_1_df)\n",
    "\n",
    "y_train_1 = ohe1.transform(y_train_1)\n",
    "\n",
    "# ohe1 객체에 담긴 인코딩 정보가 ohe1.pickle에 저장\n",
    "with open('./data/mecab/ohe1.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe1, handle)\n",
    "\n",
    "\n",
    "# 중분류 원-핫 인코딩\n",
    "\n",
    "ohe2 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe2.fit(digit_2_df)\n",
    "\n",
    "y_train_2 = ohe2.transform(y_train_2)\n",
    "\n",
    "# ohe2 객체에 담긴 인코딩 정보가 ohe2.pickle에 저장\n",
    "with open('./data/mecab/ohe2.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe2, handle)\n",
    "\n",
    "\n",
    "# 소분류 원-핫 인코딩\n",
    "\n",
    "ohe3 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe3.fit(digit_3_df)\n",
    "\n",
    "y_train_3 = ohe3.transform(y_train_3)\n",
    "\n",
    "# ohe3 객체에 담긴 인코딩 정보가 ohe3.pickle에 저장\n",
    "with open('./data/mecab/le3.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe3, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]C:\\Users\\Huitaek\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000000/1000000 [06:05<00:00, 2735.62it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'S'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-e57f2f1f0aba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-58c548d49e3c>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[1;34m(examples, labels, max_seq_len, tokenizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mdata_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'S'"
     ]
    }
   ],
   "source": [
    "train_X, train_y_1 = convert_examples_to_features(train['text'], y_train_1, max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = convert_examples_to_features(test['text'], test['digit_1'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "099c82bb4ace82ade8ea00d06692c6005c84f3239a96c9207fc6992d929102eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
