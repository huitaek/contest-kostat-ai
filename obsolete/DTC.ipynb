{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사결정나무분류 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re               # Regular Expression\n",
    "import pickle           # 토큰화된 단어목록의 인덱스를 저장, 불러오기 위해 사용\n",
    "from tqdm import tqdm   # 진행상황 Progress Bar를 위한 tqdm library\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 및 분할된 데이터셋 불러오기\n",
    "\n",
    "X_train = np.array(pd.read_csv('./data/mecab/X_train.csv'))\n",
    "y_train_1 = pd.DataFrame(pd.read_csv('./data/mecab/y_train.csv')['digit_1'])\n",
    "y_train_2 = pd.DataFrame(pd.read_csv('./data/mecab/y_train.csv')['digit_2'].astype(object))\n",
    "y_train_3 = pd.DataFrame(pd.read_csv('./data/mecab/y_train.csv')['digit_3'].astype(object))\n",
    "\n",
    "X_valid = np.array(pd.read_csv('./data/mecab/X_valid.csv'))\n",
    "y_valid_1 =  pd.DataFrame(pd.read_csv('./data/mecab/y_valid.csv')['digit_1'])\n",
    "y_valid_2 =  pd.DataFrame(pd.read_csv('./data/mecab/y_valid.csv')['digit_2'].astype(object))\n",
    "y_valid_3 =  pd.DataFrame(pd.read_csv('./data/mecab/y_valid.csv')['digit_3'].astype(object))\n",
    "\n",
    "X_test = np.array(pd.read_csv('./data/mecab/X_test.csv'))\n",
    "\n",
    "\n",
    "# 저장된 Tokenizer 객체를 불러오는 부분\n",
    "\n",
    "with open('./data/mecab/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국표준산업분류 딕셔너리 불러오기\n",
    "\n",
    "with open('./data/dictionary/digit_1_dict.pickle', 'rb') as handle:\n",
    "    digit_1_dict = pickle.load(handle)\n",
    "\n",
    "with open('./data/dictionary/digit_2_dict.pickle', 'rb') as handle:\n",
    "    digit_2_dict = pickle.load(handle)\n",
    "\n",
    "with open('./data/dictionary/digit_3_dict.pickle', 'rb') as handle:\n",
    "    digit_3_dict = pickle.load(handle)\n",
    "\n",
    "# 레이블 인코딩을 위한 산업분류 리스트, 데이터 프레임 만들기\n",
    "\n",
    "digit_1_list = list(digit_1_dict.keys())\n",
    "digit_1_df = pd.DataFrame([], columns=['digit_1'], index=[0])\n",
    "for i in range(0, len(digit_1_dict)):\n",
    "    digit_1_df.loc[i, 'digit_1'] = digit_1_list[i]\n",
    "\n",
    "digit_2_list = list(digit_2_dict.keys())\n",
    "digit_2_df = pd.DataFrame([], columns=['digit_2'], index=[0])\n",
    "for i in range(0, len(digit_2_dict)):\n",
    "    digit_2_df.loc[i, 'digit_2'] = digit_2_list[i]\n",
    "\n",
    "digit_3_list = list(digit_3_dict.keys())\n",
    "digit_3_df = pd.DataFrame([], columns=['digit_3'], index=[0])\n",
    "for i in range(0, len(digit_3_dict)):\n",
    "    digit_3_df.loc[i, 'digit_3'] = digit_3_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류 원-핫 인코딩\n",
    "\n",
    "ohe1 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe1.fit(digit_1_df)\n",
    "\n",
    "y_train_1 = ohe1.transform(y_train_1)\n",
    "y_valid_1 = ohe1.transform(y_valid_1)\n",
    "\n",
    "# ohe1 객체에 담긴 인코딩 정보가 ohe1.pickle에 저장\n",
    "with open('./data/mecab/ohe1.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe1, handle)\n",
    "\n",
    "\n",
    "# 중분류 원-핫 인코딩\n",
    "\n",
    "ohe2 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe2.fit(digit_2_df)\n",
    "\n",
    "y_train_2 = ohe2.transform(y_train_2)\n",
    "y_valid_2 = ohe2.transform(y_valid_2)\n",
    "\n",
    "# ohe2 객체에 담긴 인코딩 정보가 ohe2.pickle에 저장\n",
    "with open('./data/mecab/ohe2.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe2, handle)\n",
    "\n",
    "\n",
    "# 소분류 원-핫 인코딩\n",
    "\n",
    "ohe3 = preprocessing.OneHotEncoder(sparse=False, dtype=int)\n",
    "ohe3.fit(digit_3_df)\n",
    "\n",
    "y_train_3 = ohe3.transform(y_train_3)\n",
    "y_valid_3 = ohe3.transform(y_valid_3)\n",
    "\n",
    "# ohe3 객체에 담긴 인코딩 정보가 ohe3.pickle에 저장\n",
    "with open('./data/mecab/le3.pickle', 'wb') as handle:\n",
    "    pickle.dump(ohe3, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 973078528 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-655e541ae046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_utils.pyx\u001b[0m in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 973078528 bytes"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=200)\n",
    "model.fit(X_train, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998035"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87018"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_valid, y_valid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "099c82bb4ace82ade8ea00d06692c6005c84f3239a96c9207fc6992d929102eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
